{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algoritmo Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import pandas as pd\n",
    "\n",
    "from spacy.lang.pt import Portuguese\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing custom functions (temp)\n",
    "current_dir = os.path.abspath('')\n",
    "parent_dir = os.path.dirname(current_dir)\n",
    "parent_dir = os.path.dirname(parent_dir)\n",
    "sys.path.append(parent_dir)\n",
    "\n",
    "from helpers.string_cleaner import StringCleaner as sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Processamento Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 Extrair dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_ds = pd.read_csv('datasets/base/true.csv', sep=';')\n",
    "fake_ds = pd.read_csv('datasets/base/fake.csv', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_ds['target'] = 0\n",
    "fake_ds['target'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_df = pd.DataFrame(fake_ds)\n",
    "true_df = pd.DataFrame(true_ds)\n",
    "\n",
    "df = pd.concat([fake_df, true_ds])\n",
    "df = df.drop('idx', axis=1)\n",
    "\n",
    "# pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Limpar o dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observações**:\n",
    "* Verificar a necessidade de adicionar as siglas dos estados brasileiros como stop words.\n",
    "* Verificar a necessidade de remover o conteúdo entre colchetes e parênteses.\n",
    "* Verificar a necessidade de remover formatação de datas e horários\n",
    "* Verificar a necessidade de remover caracteres especiais, como: cedilha, acentos, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stop words using spaCy\n",
    "nlp_pt = Portuguese()\n",
    "stopwords = nlp_pt.Defaults.stop_words\n",
    "# nlp_pt.Defaults.stop_words.remove('sem')\n",
    "# nlp_pt.Defaults.stop_words.add('to')\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    final_text = []\n",
    "    for token in text.split():\n",
    "      if token.lower() not in stopwords:\n",
    "          final_text.append(token)\n",
    "    return \" \".join(final_text)\n",
    "\n",
    "def denoise_text(text):\n",
    "    text = sc.remove_url(text)\n",
    "    text = sc.remove_special_characters(text)\n",
    "    text = remove_stopwords(text)\n",
    "\n",
    "    return text.lower()\n",
    "\n",
    "df['full_text'] = df['full_text'].apply(denoise_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>full_text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>katia abreu colocar expulsao moldura nao recla...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dr ray peita bolsonaro chama conservador fake ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>reinaldo azevedo desmascarado policia federal ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>relatorio assustador bndes mostra dinheiro pub...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>radialista americano fala pt vendem ilusao bra...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>willian waack continuara internado sirio liban...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>problema cardiaco tira william waack jornal gl...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>edir macedo podera disputar eleicoes presidenc...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>boechat lula alma honesta governo diabo lula d...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>pt lula nao dinheiro sobreviver chama moro men...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>numero fuzilados durante regime fidel castro c...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>coodenador lava jato revolta aumento impostos ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>lula voce nao moleque sofreu dono friboi gosta...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>urgente janot pede prisao aecio neves procurad...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>lula recebe intimacao justica escreve letras e...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>americano revoltado escreve texto viraliza int...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>ministro defesa nao descarta intervencao milit...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>diogo portugal valioso lula viu eike corre com...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>internauta rebate texto americano criticou bra...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>pau chico francisco tambem nao costumamos repr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            full_text  target\n",
       "0   katia abreu colocar expulsao moldura nao recla...       1\n",
       "1   dr ray peita bolsonaro chama conservador fake ...       1\n",
       "2   reinaldo azevedo desmascarado policia federal ...       1\n",
       "3   relatorio assustador bndes mostra dinheiro pub...       1\n",
       "4   radialista americano fala pt vendem ilusao bra...       1\n",
       "5   willian waack continuara internado sirio liban...       1\n",
       "6   problema cardiaco tira william waack jornal gl...       1\n",
       "7   edir macedo podera disputar eleicoes presidenc...       1\n",
       "8   boechat lula alma honesta governo diabo lula d...       1\n",
       "9   pt lula nao dinheiro sobreviver chama moro men...       1\n",
       "10  numero fuzilados durante regime fidel castro c...       1\n",
       "11  coodenador lava jato revolta aumento impostos ...       1\n",
       "12  lula voce nao moleque sofreu dono friboi gosta...       1\n",
       "13  urgente janot pede prisao aecio neves procurad...       1\n",
       "14  lula recebe intimacao justica escreve letras e...       1\n",
       "15  americano revoltado escreve texto viraliza int...       1\n",
       "16  ministro defesa nao descarta intervencao milit...       1\n",
       "17  diogo portugal valioso lula viu eike corre com...       1\n",
       "18  internauta rebate texto americano criticou bra...       1\n",
       "19  pau chico francisco tambem nao costumamos repr...       1"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3 Aplicar tokenização"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observações**:\n",
    "* Testar tokenização posteriormente com NLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_dataframe(dataframe):\n",
    "    result = []\n",
    "\n",
    "    for item in dataframe:\n",
    "        tokenized_item = word_tokenize(item, language=\"portuguese\")\n",
    "\n",
    "        for token in tokenized_item:\n",
    "            result.append(token)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Aplicar Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Dividindo o dataset e transformando em um array vetorizado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    df['full_text'],\n",
    "    df['target'],\n",
    "    random_state=0,\n",
    "    test_size=0.4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ngram_range (1,1) -> unigram\n",
    "# ngram_range (2,2) -> bigram\n",
    "# ngram_range (1,2) -> unigram and bigram\n",
    "\n",
    "vectorizer = CountVectorizer(ngram_range=(1,2)).fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_vectorized = vectorizer.transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train_vectorized.toarray().shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Criando um modelo de Naive Bayes e o treinando com o dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=0.1)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MultinomialNB(alpha=0.1)\n",
    "model.fit(X_train_vectorized, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 Realizando uma predição"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_predict = model.predict(vectorizer.transform(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.5 Observando a precisão"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79.55246913580247 %\n"
     ]
    }
   ],
   "source": [
    "accuracy = accuracy_score(Y_test, Y_predict)*100\n",
    "print(str(accuracy) + \" %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.6 Métricas mais detalhadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.98      0.83      1294\n",
      "           1       0.97      0.61      0.75      1298\n",
      "\n",
      "    accuracy                           0.80      2592\n",
      "   macro avg       0.84      0.80      0.79      2592\n",
      "weighted avg       0.84      0.80      0.79      2592\n",
      "\n"
     ]
    }
   ],
   "source": [
    "report = classification_report(Y_test, Y_predict)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4 Executando uma matrix de confusão"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1270   24]\n",
      " [ 506  792]]\n"
     ]
    }
   ],
   "source": [
    "cm = confusion_matrix(Y_test, Y_predict)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4 Testando o modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, 0, 0])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(vectorizer.transform(\n",
    "    [\n",
    "        \"O Podemos decidiu  expulsar o deputado federal Carlos Gaguim do partido após a Polícia Federal fazer buscas a apreensões no gabinete dele na Câmara. Com isso, a legenda abre espaço para receber a senadora expulsa pelo PMDB, Katia Abreu. Por meio de nota, a legenda informou que o afastamento do parlamentar já era algo acordado entre os filiados da sigla.  “Ainda que o parlamentar tenha comunicado a conclusão de sua desfiliação para esta semana, diante dos fatos noticiados hoje, a Executiva Nacional do Podemos solicita o imediato cancelamento de sua filiação dos quadros do partido”. O partido, que no passado chegou a cogitar lançar o parlamentar como candidato ao Senado, diz que “apoia a investigação com a ampla apuração dos eventuais crimes cometidos e a consequente responsabilização dos envolvidos, para que todos sejam punidos com o máximo rigor da Lei, independentemente de posição ou cargo ocupado\",\n",
    "        \"Vota e manda pra todos os Bolsonaristas que vc conhece, família, etc.  Peça que façam o mesmo (votar e repassar apenas pra Bolsonaristas).  Essa é uma enquete da jovem pan e o resultado vai ser divulgado a segunda feira.  Fizeram uma primeira e deu 91% Bolsonaro e 9 Lula.  Pessoas da esquerda reclamaram e a jovem pan resolveu fazer essa segunda.  A esquerda se mobilizou, divulgaram e o Lula cresceu bastante.  Nesse momento tá 64 Bolsonaro e 36 Lula.  Vamos nos mobilizar também e golear de novo.\",\n",
    "        \"Justiça mandou devolver mais de 600 kg de cocaína para traficantes no Rio de Janeiro\",\n",
    "        \"Justiça obrigou a devolução de mais de 550 kg de cocaína para traficantes no estado do Rio\",\n",
    "        \"Professor de Geografia da Escola Estadual Camilo Dias faz apologia ao crime em sala de aula. Sem notar alunos gravaram ele falando coisas absurdas, uma mãe fez o seguinte relato Tô chocada, o professor é pago para dar aula de geografia e não a desmotivar  alunos de 15 anos estudar e trabalhar. Ele tá colocando na cabeça dos adolescentes que não é errado roubar e que é uma profissão.  Que traficante, prostituta e ladrão são cidadãos de bem, pais de família e que são profissionais relatou mãe qu pede providências das autoridades. E vc pais oq acham deste profissional de educação? Escute bem o que o professor fala para os alunos.\"\n",
    "    ]\n",
    "))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "db0f6cbdde3eb805f0a229b988d6bd72d452c42e640dd9958b4355934ec01bf5"
  },
  "kernelspec": {
   "display_name": "Python 3.6.15 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
